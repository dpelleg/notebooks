{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import pdfhelper\n",
    "\n",
    "charts = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate data, run this: python3 readpdf.py  -C\n",
    "tbs = pd.read_csv('cells.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if charts:\n",
    "    sns.displot(tbs, x=\"dx_raw\", hue='col_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if charts:\n",
    "    sns.displot(tbs, x=\"len\", hue='col_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if charts:\n",
    "    features = ['int', 'starts_ws', 'ends_ws', 'all_letters']\n",
    "    fig, axes = plt.subplots(len(features))\n",
    "\n",
    "    for i, f in enumerate(features):\n",
    "        sns.displot(tbs, x=f, hue='col_idx', ax=axes[i]).set(title=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pdfhelper.preprocess_tbs_data(tbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the output variable\n",
    "#y = cells['col']\n",
    "#y = cells['col_idx']\n",
    "#X = cells.drop(columns=['col', 'col_idx'])\n",
    "\n",
    "# drop input variables which only complicate things\n",
    "#to_drop = ['y', 'text']\n",
    "#X = X.drop(to_drop,axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_vars = list(X_train.select_dtypes(include = float).columns)\n",
    "\n",
    "cont_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy = 'median'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "# test to make sure the pipeline works\n",
    "pd.DataFrame(cont_pipeline.fit_transform(X_train[cont_vars]), columns = cont_vars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_vars = list(X_train.select_dtypes(include = int).columns)\n",
    "\n",
    "if disc_vars:\n",
    "    disc_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy = 'constant', fill_value = -1)\n",
    "    )\n",
    "\n",
    "    pd.DataFrame(disc_pipeline.fit_transform(train[disc_vars]), columns = disc_vars);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_vars = []\n",
    "\n",
    "if cat_vars:\n",
    "    cat_pipeline = make_pipeline(\n",
    "        SimpleImputer(strategy = 'constant', fill_value = 'unknown'),\n",
    "        OneHotEncoder()\n",
    "    )\n",
    "\n",
    "    cat_pipeline.fit_transform(train[cat_vars]);\n",
    "    #cat_pipeline.named_steps['onehotencoder'].get_feature_names(['original_lang','release_season']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('continuous', cont_pipeline, cont_vars)\n",
    "#        ('discrete', disc_pipeline, disc_vars),\n",
    "#        ('categorical', cat_pipeline, cat_vars),\n",
    "#        ('json', json_pipeline, json_vars)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "                     #cat_pipeline.named_steps['onehotencoder'].get_feature_names(['original_lang','release_season']),\n",
    "                      #json_pipeline.named_steps['topcatencoder'].get_feature_names()))\n",
    "        \n",
    "preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoLars\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "\n",
    "def quick_eval(pipeline, X, y, verbose=True):\n",
    "    \"\"\"\n",
    "    Quickly trains modeling pipeline and evaluates on test data. Returns original model and cross-validation score\n",
    "    as a tuple.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = cross_val_score(pipeline, X, y, cv=10, scoring='accuracy')\n",
    "    score = mean(scores)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Algorithm: {pipeline.named_steps['classifier'].__class__.__name__}\")\n",
    "        print(f\"CV score: {score}\")\n",
    "    \n",
    "    return pipeline.named_steps['classifier'], score\n",
    "\n",
    "ccp_alpha=0 # 0.03\n",
    "\n",
    "classifiers = [LogisticRegression(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "classifiers = [DecisionTreeClassifier(max_depth=n, ccp_alpha=ccp_alpha) for n in [10,15,20,25]]\n",
    "regressors = [\n",
    "    LinearRegression(),\n",
    "    Lasso(alpha=.5),\n",
    "    Ridge(alpha=.1),\n",
    "    LassoLars(alpha=.1),\n",
    "    DecisionTreeRegressor(),\n",
    "    RandomForestRegressor(),\n",
    "    AdaBoostRegressor(),\n",
    "    GradientBoostingRegressor()\n",
    "]\n",
    "\n",
    "for r in classifiers:\n",
    "    pipe = Pipeline(steps = [\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', r)\n",
    "    ])\n",
    "\n",
    "    quick_eval(pipe, X_train, y_train)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = DecisionTreeClassifier(max_depth=25, ccp_alpha=0.03)\n",
    "pipe = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', r)\n",
    "])\n",
    "m = pipe.fit(X_train, y_train)\n",
    "clf = m.named_steps['classifier']\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nms = list(m.named_steps['preprocessor'].get_feature_names_out())\n",
    "t = sklearn.tree.export_text(m.named_steps['classifier'], feature_names=nms)\n",
    "print(\"{} nodes\".format(clf.tree_.node_count))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('col_classifier.pkl', 'wb') as file:\n",
    "    pickle.dump(pipe, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in X_test.iterrows():\n",
    "    row_dict = row.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.read_csv('table.csv')\n",
    "#f = 'מספר בקשה'\n",
    "#p.rename(columns={f: 'n'}, inplace=True)\n",
    "#p.query('n == 950')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality check\n",
    "for c in p.columns:\n",
    "    print(c)\n",
    "    print(p[c].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
