{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "_cv5WLoLQVD5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline \n",
    "#Load hospital data\n",
    "merged_data = pd.read_pickle(\"data/merged_data_7May2020.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1224,
     "status": "ok",
     "timestamp": 1589739147335,
     "user": {
      "displayName": "Dan P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxZkksSJl6C3pFfB_0qES5NDq-LIAp14ZGehqe=s64",
      "userId": "13599913014647747669"
     },
     "user_tz": -180
    },
    "id": "Njy8rCgUyOYN",
    "outputId": "cdc01cf8-ea57-4fbd-9a65-80b242ab93ad",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AssafHarofe' 'BneiZion' 'Barzilai' 'HilelYaffe' 'Galil' 'Volfson' 'Ziv'\n",
      " 'Poriya' 'Rambam' 'Shiba']\n"
     ]
    }
   ],
   "source": [
    "print(merged_data['hospital'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hospital', 'Date', 'keep', 'hospitalization_0_1',\n",
       "       'hospitalization_1_4', 'hospitalization_5_18', 'hospitalization_19_34',\n",
       "       'hospitalization_35_64', 'hospitalization_65_74',\n",
       "       'hospitalization_75_84', 'hospitalization_85plus',\n",
       "       'all_hospitalizations', 'release_0_1', 'release_1_4', 'release_5_18',\n",
       "       'release_19_34', 'release_35_64', 'release_65_74', 'release_75_84',\n",
       "       'release_85plus', 'all_releases', 'all_visits'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = merged_data\n",
    "m.rename(columns = lambda x : re.sub('\\s*:\\s*', '_', re.sub('-','_', x)), inplace=True)\n",
    "m.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qya6zTxbmS4L"
   },
   "outputs": [],
   "source": [
    "#Run a smoothing filter on the hospitalization data\n",
    "from scipy import signal\n",
    "b, a = signal.butter(14, 0.07)\n",
    "\n",
    "if (1):\n",
    "  smoothed_merged_data = merged_data\n",
    "\n",
    "  for column in smoothed_merged_data:\n",
    "    if (column != \"Date\") and (column != \"hospital\"):\n",
    "      smoothed_merged_data[column] = signal.filtfilt(b, a, smoothed_merged_data[column], padlen=7)\n",
    "      #smoothed_merged_data[column] = smoothed_merged_data[column].rolling(window=14).mean()\n",
    "\n",
    "  merged_data = smoothed_merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4512,
     "status": "ok",
     "timestamp": 1589739162127,
     "user": {
      "displayName": "Dan P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxZkksSJl6C3pFfB_0qES5NDq-LIAp14ZGehqe=s64",
      "userId": "13599913014647747669"
     },
     "user_tz": -180
    },
    "id": "eLoMYN_7USzs",
    "outputId": "1a8401a0-557b-4bce-ecdd-bea68ca5183a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "air_quality = pd.read_excel(\"data/FormattedStationData.xlsx\")\n",
    "\n",
    "# there's a stray SO2 sensor for Kiryat Haim for some reason, remove it\n",
    "air_quality = air_quality.drop('Kiryat Haim : SO2', axis='columns').rename(columns = {'Kiryat Haim : SO2.1' : 'Kiryat Haim : SO2'})\n",
    "\n",
    "air_quality[\"DateTime\"] = pd.to_datetime(air_quality[\"Date-Time\"],errors='coerce')\n",
    "air_quality[\"PollutionDate\"] = air_quality[\"DateTime\"].dt.date\n",
    "\n",
    "#Check if all dates have been processed (should be zero)\n",
    "print(len(air_quality[\"DateTime\"]) - air_quality[\"DateTime\"].count())\n",
    "\n",
    "#print(air_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read air quality sensor data\n",
    "air_quality = pd.read_excel(\"data/StationData.xlsx\")\n",
    "\n",
    "# Many columns are empty or sparse. Require 80% availability of data per column\n",
    "air_quality.dropna(thresh=len(air_quality)*0.8, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shprintzak__O3', 'Shprintzak__NO2', 'Shprintzak__NO', 'Shprintzak__NOX', 'Romema__CO', 'Romema__NO2', 'Romema__NO', 'Romema__NOX', 'Kiryat_Yam__NOX', 'Kiryat_Yam__NO', 'Kiryat_Yam__NO2', 'Kiryat_Yam__O3', 'Kiryat_Tivon__NO2', 'Kiryat_Tivon__O3', 'Kiryat_Tivon__NO', 'Kiryat_Tivon__NOX', 'Kiryat_Tivon__SO2', 'Kiryat_Haim_Regavim__SO2', 'Kiryat_Haim_Regavim__NO', 'Kiryat_Haim_Regavim__NO2', 'Kiryat_Haim_Regavim__BENZN', 'Kiryat_Haim_Regavim__NOX', 'Kiryat_Haim_Regavim__O_Xyle', 'Kiryat_Haim_Regavim__TOLUEN', 'Kiryat_Haim_Regavim__EthylB', 'Kiryat_Binyamin__EthylB', 'Kiryat_Binyamin__O_Xyle', 'Kiryat_Binyamin__NOX', 'Kiryat_Binyamin__TOL', 'Kiryat_Binyamin__BENZN', 'Kiryat_Binyamin__NO', 'Kiryat_Binyamin__NO2', 'Kiryat_Binyamin__SO2', 'Kiryat_Ata__NO2', 'Kiryat_Ata__O3', 'Kiryat_Ata__NOX', 'Kiryat_Ata__SO2', 'Kiryat_Ata__NO', 'Park_Carmel__O3', 'Park_Carmel__NOx', 'Park_Carmel__NO', 'Park_Carmel__SO2', 'Atzmaut__NO2', 'Atzmaut__CO', 'Atzmaut__NOX', 'Atzmaut__NO', 'Nesher__NOX', 'Nesher__SO2', 'Nesher__NO', 'Nesher__O3', 'Nesher__NO2', 'Neve_Shaanan__CO', 'Neve_Shaanan__NO', 'Neve_Shaanan__NOX', 'Neve_Shaanan__SO2', 'Neve_Shaanan__O3', 'Neve_Shaanan__NO2', 'Neve_Yosef__NO', 'Neve_Yosef__NO2', 'Neve_Yosef__NOX', 'Carmelia__NO', 'Carmelia__NOX', 'Carmelia__NO2', 'Kfar_Hasidim__NO', 'Kfar_Hasidim__NOX', 'Kfar_Hasidim__O3', 'Kfar_Hasidim__NO2', 'Kfar_Hasidim__SO2', 'Yizraelia__NOX', 'Yizraelia__CO', 'Yizraelia__NO2', 'Yizraelia__NO', 'Igud__check_post___BENZN', 'Igud__check_post___NO', 'Igud__check_post___NO2', 'Igud__check_post___O3', 'Igud__check_post___SO2', 'Igud__check_post___NOX', 'Igud__check_post___H2S', 'Igud__check_post___EthylB', 'Igud__check_post___TOLUEN', 'Hugim__SO2', 'Hugim__NO', 'Hugim__NO2', 'D.CARMEL__SO2', 'D.CARMEL__NOX', 'D.CARMEL__NO', 'D.CARMEL__NO2', 'Einstein__SO2', 'Einstein__NOX', 'Einstein__NO2', 'Einstein__NO', 'Ahuza__NO2', 'Ahuza__NO', 'Ahuza__NOX']\n",
      "['Romema__PM10', 'Kiryat_Tivon__PM25', 'Kiryat_Haim_Regavim__PM10', 'Kiryat_Binyamin__PM25', 'Park_Carmel__DUST', 'Atzmaut__PM1', 'Atzmaut__PM25', 'Nesher__PM25', 'Neve_Shaanan__PM10', 'Neve_Shaanan__PM25', 'Carmelia__PM10', 'Yizraelia__PM10', 'Igud__check_post___PM25', 'Igud__check_post___PM10', 'Ahuza__PM10', 'Ahuza__PM25']\n"
     ]
    }
   ],
   "source": [
    "# Mark column be type of pollutants they measure\n",
    "\n",
    "import re\n",
    "\n",
    "air_quality.rename(columns=lambda x: re.sub('[- ()+]','_', re.sub('2\\.5', '25', re.sub('_', '__', x))), inplace=True)\n",
    "\n",
    "colre_gaseous = re.compile('(O3|NO2|NO|NOX|SO2|TOC_SCRUBBER|TOC_RTO|CO|BENZN|O_Xyle|TOLUEN|EthylB|M_P_XY|1_3butadiene|TOL|NOx|H2S|CO TRAFIC|NO2_TRAFIC|NO_TRAFIC|NOX_TRAFIC)$')\n",
    "\n",
    "colre_particulate = re.compile('(PM10|PM25|pm\\(10-2\\.5\\)|pm_10_25_|DUST|PM1|BLACK CARBON)$')\n",
    "\n",
    "colre_climate = re.compile('(StWd|RH|WDS|WDD|SR|PREC)$')  # not used, we take climate data from another source\n",
    "colre_operational = re.compile('(ITemp|TEMP|Filter|LXpk_max|LAF_max|LAF_min|LAim|LXeq|LAeq|FILTER_2.5|spare|PM1_Flow|PM25_Filter|PM10_Noise|PM25_Noise|PM10_Filter|Flow)$')  # not used, this should not matter for anythingg\n",
    "\n",
    "# There are two data columns which didn't seem to belong anywhere: BP and BRP. Anyone knows what they are?2\n",
    "\n",
    "all_cols = air_quality.columns\n",
    "\n",
    "aircols_gaseous = list(filter(colre_gaseous.search, all_cols))\n",
    "aircols_particulate = list(filter(colre_particulate.search, all_cols))\n",
    "\n",
    "print(aircols_gaseous)\n",
    "print(aircols_particulate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dakauXR7wjmR"
   },
   "outputs": [],
   "source": [
    "#Run a smoothing filter on the air quality data\n",
    "from scipy import signal\n",
    "b, a = signal.butter(14, 0.07)\n",
    "\n",
    "if (0):\n",
    "  smoothed_filtered_air_quality = filtered_air_quality\n",
    "  smoothed_filtered_air_quality = smoothed_filtered_air_quality.fillna(-1)\n",
    "\n",
    "  for column in smoothed_filtered_air_quality:\n",
    "    if (column != \"PollutionDate\") and (column != \"Date-Time\") and (column != \"DateTime\"): \n",
    "      smoothed_filtered_air_quality[column] = signal.filtfilt(b, a, smoothed_filtered_air_quality[column], padlen=14)\n",
    "      #smoothed_filtered_air_quality[column] = smoothed_filtered_air_quality[column].rolling(window=14).mean()\n",
    "\n",
    "  filtered_air_quality = smoothed_filtered_air_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 62120,
     "status": "ok",
     "timestamp": 1589739258074,
     "user": {
      "displayName": "Dan P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxZkksSJl6C3pFfB_0qES5NDq-LIAp14ZGehqe=s64",
      "userId": "13599913014647747669"
     },
     "user_tz": -180
    },
    "id": "neVSqQcmWPci",
    "outputId": "3518d01b-fa29-408d-b9dc-a114f69813cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr   \n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "sep = ' + '\n",
    "\n",
    "#lag = 0\n",
    "hospital = 'Rambam'\n",
    "\n",
    "weather_cols = [col for col in filtered_air_quality.columns if 'Date' not in col]\n",
    "\n",
    "formula = sep.join([\"all_visits ~ 1 \", sep.join(weather_cols)])\n",
    "#print(formula)\n",
    "\n",
    "correlations = pd.DataFrame(columns=['Lag', 'Cor'])\n",
    "\n",
    "for lag in range(-60, 60):\n",
    "  filtered_air_quality[\"ShiftedPollutionDate\"] = pd.DatetimeIndex(filtered_air_quality[\"PollutionDate\"]) + pd.DateOffset(lag);\n",
    "  merged_all = pd.merge(merged_data, filtered_air_quality, how='left', left_on='Date', right_on='ShiftedPollutionDate')\n",
    "  #merged_all = merged_all.fillna(-1)\n",
    "\n",
    "  #Split to train and test\n",
    "  train_row_mask = (merged_all[\"hospital\"] == hospital) & (merged_all['Date'] <= '2018-12-31')\n",
    "  test_row_mask  = (merged_all[\"hospital\"] == hospital) & (merged_all['Date'] > '2018-12-31')\n",
    "\n",
    "  y, X = dmatrices(formula, merged_all)\n",
    "  y_train = y[train_row_mask]\n",
    "  X_train = X[train_row_mask]\n",
    "  y_test = y[test_row_mask]\n",
    "  y_test = [ i[0] for i in y_test]\n",
    "  X_test = X[test_row_mask]\n",
    "\n",
    "  #rf = RandomForestRegressor(n_estimators = 10)\n",
    "  #rf.fit(X_train, y_train);\n",
    "  #pred = rf.predict(X_test)\n",
    "\n",
    "  model = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\n",
    "  pred  = model.predict(X_test.astype(float))\n",
    "\n",
    "  #curCorr = y_test.corr(pred)\n",
    "\n",
    "  curCorr = pearsonr(pred, y_test)\n",
    "  curCorr = curCorr[0]\n",
    "\n",
    "  correlations = correlations.append({'Lag': lag, 'Cor': curCorr}, ignore_index=True)\n",
    "  print(\".\", end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1589739259067,
     "user": {
      "displayName": "Dan P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxZkksSJl6C3pFfB_0qES5NDq-LIAp14ZGehqe=s64",
      "userId": "13599913014647747669"
     },
     "user_tz": -180
    },
    "id": "9tseCKmjfTWJ",
    "outputId": "57ddb50f-f650-43ae-8eff-3af0b47252f4"
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "b, a = signal.butter(5, 0.2)\n",
    "\n",
    "correlations2 = correlations\n",
    "y = signal.filtfilt(b, a, correlations2['Cor'], padlen=5)\n",
    "correlations2['Cor'] = y\n",
    "#print(correlations2['Cor'])\n",
    "\n",
    "correlations2.plot(kind='scatter',x='Lag',y='Cor',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "Xlzdm0gIs702"
   },
   "outputs": [],
   "source": [
    "#Read weather and inversion data\n",
    "weather = pd.read_csv(\"data/weather.csv\")\n",
    "\n",
    "weather.rename(columns=lambda x: re.sub('[- ()+]','_', x), inplace=True)\n",
    "\n",
    "weather[\"date\"] = pd.to_datetime(weather[\"date\"],errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join all independent variables into one table\n",
    "all_indep = air_quality.merge(weather, how='inner', left_on= \"DateTime\", right_on=\"date\")\n",
    "print(air_quality[['DateTime','Date_Time', 'PollutionDate']].head())\n",
    "print(weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 41602,
     "status": "ok",
     "timestamp": 1589739300697,
     "user": {
      "displayName": "Dan P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxZkksSJl6C3pFfB_0qES5NDq-LIAp14ZGehqe=s64",
      "userId": "13599913014647747669"
     },
     "user_tz": -180
    },
    "id": "rzNPk7lEtSHE",
    "outputId": "045ed3b6-f0ca-4baf-846c-14bc48334e19"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "sep = ' + '\n",
    "\n",
    "#lag = 0\n",
    "hospital = 'Rambam'\n",
    "\n",
    "cols = [col for col in inversion.columns if 'date' not in col]\n",
    "\n",
    "formula = sep.join([\"all_visits ~ 1\", sep.join(cols)])\n",
    "print(formula)\n",
    "\n",
    "correlations = pd.DataFrame(columns=['Lag', 'Cor'])\n",
    "\n",
    "for lag in range(-60, 60):\n",
    "#for lag in range(-6, 6):\n",
    "  inversion[\"ShiftedPollutionDate\"] = pd.DatetimeIndex(inversion[\"date\"]) + pd.DateOffset(lag)\n",
    "\n",
    "  merged_all = pd.merge(merged_data, inversion, how='left', left_on='Date', right_on='ShiftedPollutionDate')\n",
    "  merged_all = merged_all.fillna(-1)\n",
    "\n",
    "  #Split to train and test\n",
    "  train_row_mask = (merged_all[\"hospital\"] == hospital) & (merged_all['Date'] <= '2018-12-31')\n",
    "  test_row_mask  = (merged_all[\"hospital\"] == hospital) & (merged_all['Date'] > '2018-12-31')\n",
    "\n",
    "  y, X = dmatrices(formula, merged_all)\n",
    "  y_train = y[train_row_mask]\n",
    "  X_train = X[train_row_mask]\n",
    "  y_test = y[test_row_mask]\n",
    "  y_test = [ i[0] for i in y_test]\n",
    "  X_test = X[test_row_mask]\n",
    "\n",
    "  model = sm.OLS(y_train.astype(float), X_train.astype(float)).fit()\n",
    "  pred  = model.predict(X_test.astype(float))\n",
    "\n",
    "  correlations = correlations.append({'Lag': lag, 'Cor': np.corrcoef(y_test, pred)[0,1]}, ignore_index=True)\n",
    "  print(\".\", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1589739302380,
     "user": {
      "displayName": "Dan P",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgxZkksSJl6C3pFfB_0qES5NDq-LIAp14ZGehqe=s64",
      "userId": "13599913014647747669"
     },
     "user_tz": -180
    },
    "id": "V-xcj9lyv23s",
    "outputId": "4c1c04aa-1110-49f0-e351-0eeae01fe271"
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "b, a = signal.butter(5, 0.2)\n",
    "\n",
    "correlations2 = correlations\n",
    "y = signal.filtfilt(b, a, correlations2['Cor'], padlen=5)\n",
    "correlations2['Cor'] = y\n",
    "#print(correlations2['Cor'])\n",
    "\n",
    "#correlations2['Cor'] = correlations2['Cor'].filtfilt(window=5).mean()\n",
    "\n",
    "correlations2.plot(kind='line',x='Lag',y='Cor',color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations, product\n",
    "\n",
    "inversion_basic = ['inversion_bywind', 'inversion_bytemp_delta', 'inversion_byboth']\n",
    "climate = [\"hmd_rlt_Bazan\", \"prs_lvl_hgt_Bazan\", \"tmp_air_dry_Bazan\", \"tmp_air_wet_Bazan\", \"tmp_dew_pnt_Bazan\", \"wind_dir_Bazan\", \"wind_spd_Bazan\"]\n",
    "\n",
    "\n",
    "xs = ['gaseous', 'particulate', 'climate', 'inversion']\n",
    "\n",
    "\n",
    "all_subsets = chain.from_iterable(combinations(xs,n) for n in range(1, len(xs)+1))\n",
    "\n",
    "print(list(all_subsets))\n",
    "\n",
    "xp = ['climate', 'inversion']\n",
    "\n",
    "inversion_cols = inversion_basic.copy()\n",
    "inversion_cols.extend(map(lambda x: ':'.join(x), product(inversion_basic, climate)))\n",
    "print(inversion_cols)\n",
    "\n",
    "#factor_map = { 'gaseous' : aircols_gaseous, 'particulate' : aircols_particulate, 'climate' : climate_cols, 'inversion' : inversion_cols }"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Correlate_with_weather.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
